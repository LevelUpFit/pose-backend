import matplotlib.pyplot as plt
import cv2
import mediapipe as mp
import sys
import math
import numpy as np
import tempfile
import uuid
from PIL import Image, ImageDraw, ImageFont

from app.services.video_utils_ver2 import rotated_frame_generator

# í°íŠ¸ ìºì‹± (ë§¤ í”„ë ˆì„ë§ˆë‹¤ ë¡œë“œí•˜ì§€ ì•Šë„ë¡)
_font_cache = {}

def get_korean_font(font_size=20):
    """í•œê¸€ í°íŠ¸ë¥¼ ìºì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if font_size in _font_cache:
        return _font_cache[font_size]
    
    font = None
    # Windows, Linux, macOS í°íŠ¸ ê²½ë¡œë“¤
    font_paths = [
        # Windows
        "C:/Windows/Fonts/malgun.ttf",      # ë§‘ì€ ê³ ë”•
        "C:/Windows/Fonts/malgunbd.ttf",    # ë§‘ì€ ê³ ë”• Bold
        "C:/Windows/Fonts/gulim.ttc",       # êµ´ë¦¼
        "C:/Windows/Fonts/batang.ttc",      # ë°”íƒ•
        "C:/Windows/Fonts/ngulim.ttf",      # ìƒˆêµ´ë¦¼
        # Linux
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        # macOS
        "/System/Library/Fonts/AppleGothic.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/Library/Fonts/NanumGothic.ttf",
    ]
    
    for font_path in font_paths:
        try:
            font = ImageFont.truetype(font_path, font_size)
            print(f"[Font] Loaded: {font_path}")
            break
        except Exception:
            continue
    
    if font is None:
        print("[Font] Warning: No Korean font found, using default")
        font = ImageFont.load_default()
    
    _font_cache[font_size] = font
    return font

def put_korean_text(frame, text, position, font_size=20, color=(255, 255, 255)):
    """OpenCV í”„ë ˆì„ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # BGR -> RGB ë³€í™˜
        img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        
        # ìºì‹±ëœ í°íŠ¸ ì‚¬ìš©
        font = get_korean_font(font_size)
        
        # BGR -> RGB ìƒ‰ìƒ ë³€í™˜
        color_rgb = (color[2], color[1], color[0])
        draw.text(position, text, font=font, fill=color_rgb)
        
        # RGB -> BGR ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    except Exception as e:
        print(f"[put_korean_text] Error: {e}")
        # ì—ëŸ¬ ì‹œ ì›ë³¸ í”„ë ˆì„ ë°˜í™˜
        return frame

def annotate_lunge_video(input_path: str, output_path: str):
    # 1) ì›ë³¸ ë¹„ë””ì˜¤ì—ì„œ FPS, ê°€ë¡œÃ—ì„¸ë¡œ ê°€ì ¸ì˜¤ê¸°
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()

    # 2) VideoWriter ì¤€ë¹„ (MP4)
    # H.264 ì½”ë± ì‹œë„ (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ìµœê³ )
    writer = None
    for codec in ['avc1', 'h264', 'H264', 'x264', 'X264']:
        fourcc = cv2.VideoWriter_fourcc(*codec)
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        if writer.isOpened():
            print(f"Using codec: {codec}")
            break
    
    if writer is None or not writer.isOpened():
        # ëª¨ë“  H.264 ì½”ë± ì‹¤íŒ¨ì‹œ mp4vë¡œ í´ë°±
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        print("Fallback to mp4v codec")
    
    if not writer.isOpened():
        raise RuntimeError(f"Cannot open VideoWriter for {output_path}")

    # 3) Mediapipe í¬ì¦ˆ ëª¨ë¸ë¡œ í”„ë ˆì„ë§ˆë‹¤ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
    with mp.solutions.pose.Pose(static_image_mode=False) as pose:
        for frame in rotated_frame_generator(input_path):
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            res = pose.process(rgb)
            if res.pose_landmarks:
                mp.solutions.drawing_utils.draw_landmarks(
                    frame,
                    res.pose_landmarks,
                    mp.solutions.pose.POSE_CONNECTIONS
                )
            writer.write(frame)

    writer.release()


def extract_front_knee_foot_xs_lunge_style(frame_gen, show_video=False, save_video_path=None, fps=30):
    from app.utils.angle_utils import calculate_angle
    mp_pose = mp.solutions.pose
    knee_xs = []
    foot_xs = []
    knee_angles = []
    hip_y_list = []
    knee_y_list = []
    width, height = None, None
    video_writer = None
    total_penalty = 0
    frame_count = 0
    
    # ìˆ˜ì¶•/ì´ì™„ ë¶„ì„ìš© ë³€ìˆ˜
    diff_y_history = []  # í”„ë ˆì„ë³„ ì—‰ë©ì´-ë¬´ë¦ ë†’ì´ ì°¨ì´
    current_phase = None  # 'contraction' or 'relaxation'
    phase_start_frame = 0
    smoothing_window = 7

    with mp_pose.Pose(static_image_mode=False) as pose:
        for frame in frame_gen:
            if width is None or height is None:
                height, width = frame.shape[:2]
                if save_video_path is not None:
                    # H.264 ì½”ë± ì‹œë„ (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ìµœê³ )
                    for codec in ['avc1', 'h264', 'H264', 'x264', 'X264']:
                        fourcc = cv2.VideoWriter_fourcc(*codec)
                        video_writer = cv2.VideoWriter(save_video_path, fourcc, fps, (width, height))
                        if video_writer.isOpened():
                            print(f"Using codec: {codec}")
                            break
                    
                    if video_writer is None or not video_writer.isOpened():
                        # ëª¨ë“  H.264 ì½”ë± ì‹¤íŒ¨ì‹œ mp4vë¡œ í´ë°±
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        video_writer = cv2.VideoWriter(save_video_path, fourcc, fps, (width, height))
                        print("Fallback to mp4v codec")

            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = pose.process(image_rgb)
            if result.pose_landmarks:
                landmarks = result.pose_landmarks.landmark
                nose_x = landmarks[0].x * width
                left_shoulder_x = landmarks[11].x * width
                right_shoulder_x = landmarks[12].x * width
                left_knee_x = landmarks[25].x * width
                right_knee_x = landmarks[26].x * width
                left_foot_x = landmarks[31].x * width
                right_foot_x = landmarks[32].x * width

                # ì•ë‹¤ë¦¬ íŒë³„ ë° ì¸ë±ìŠ¤ ê²°ì •
                if nose_x > right_shoulder_x:
                    look_direction = "right"
                else:
                    look_direction = "left"

                if look_direction == "right":
                    if right_foot_x > left_foot_x:
                        knee_x = right_knee_x
                        foot_x = right_foot_x
                        front_foot_x = right_foot_x
                        hip_idx, knee_idx, ankle_idx = 24, 26, 28
                        # ìˆ˜ì§ì„  ì •ë ¬ìš© (ë°˜ëŒ€ìª½ = ì™¼ìª½)
                        opp_shoulder_idx, opp_hip_idx, opp_knee_idx = 11, 23, 25
                    else:
                        knee_x = left_knee_x
                        foot_x = left_foot_x
                        front_foot_x = left_foot_x
                        hip_idx, knee_idx, ankle_idx = 23, 25, 27
                        # ìˆ˜ì§ì„  ì •ë ¬ìš© (ë°˜ëŒ€ìª½ = ì˜¤ë¥¸ìª½)
                        opp_shoulder_idx, opp_hip_idx, opp_knee_idx = 12, 24, 26
                else:
                    if left_foot_x < right_foot_x:
                        knee_x = left_knee_x
                        foot_x = left_foot_x
                        front_foot_x = left_foot_x
                        hip_idx, knee_idx, ankle_idx = 23, 25, 27
                        # ìˆ˜ì§ì„  ì •ë ¬ìš© (ë°˜ëŒ€ìª½ = ì˜¤ë¥¸ìª½)
                        opp_shoulder_idx, opp_hip_idx, opp_knee_idx = 12, 24, 26
                    else:
                        knee_x = right_knee_x
                        foot_x = right_foot_x
                        front_foot_x = right_foot_x
                        hip_idx, knee_idx, ankle_idx = 24, 26, 28
                        # ìˆ˜ì§ì„  ì •ë ¬ìš© (ë°˜ëŒ€ìª½ = ì™¼ìª½)
                        opp_shoulder_idx, opp_hip_idx, opp_knee_idx = 11, 23, 25

                knee_xs.append(knee_x)
                foot_xs.append(foot_x)

                # ìˆ˜ì§ì„  ì •ë ¬ ê³„ì‚° (ì–´ê¹¨-ì—‰ë©ì´-ë°˜ëŒ€ìª½ ë¬´ë¦)
                opp_shoulder = (landmarks[opp_shoulder_idx].x * width, landmarks[opp_shoulder_idx].y * height)
                opp_hip = (landmarks[opp_hip_idx].x * width, landmarks[opp_hip_idx].y * height)
                opp_knee_pt = (landmarks[opp_knee_idx].x * width, landmarks[opp_knee_idx].y * height)
                vertical_angle = calc_three_point_angle(opp_shoulder, opp_hip, opp_knee_pt)
                vertical_deviation = abs(180 - vertical_angle)

                # ë¬´ë¦ì´ ë°œëë³´ë‹¤ ì•ìœ¼ë¡œ ë‚˜ê°”ëŠ”ì§€ íŒë³„ ë° íŒ¨ë„í‹° ê³„ì‚°
                if look_direction == "right":
                    over_distance = max(0, knee_x - foot_x)
                else:
                    over_distance = max(0, foot_x - knee_x)
                
                penalty = calc_penalty(over_distance, threshold=10, max_penalty=100) if over_distance > 0 else 0
                total_penalty += penalty
                frame_count += 1

                # ì•ë‹¤ë¦¬ ë¬´ë¦ ê°ë„ ê³„ì‚° ë° ë°°ì—´ ì €ì¥
                hip = (landmarks[hip_idx].x * width, landmarks[hip_idx].y * height)
                knee = (landmarks[knee_idx].x * width, landmarks[knee_idx].y * height)
                ankle = (landmarks[ankle_idx].x * width, landmarks[ankle_idx].y * height)
                knee_angle = calculate_angle(hip, knee, ankle)
                knee_angles.append(knee_angle)
                hip_y_list.append(int(hip[1]))
                knee_y_list.append(int(knee[1]))

                # ìˆ˜ì¶•/ì´ì™„ ë¶„ì„ìš© ë°ì´í„° ì €ì¥
                current_diff = abs(int(knee[1]) - int(hip[1]))
                diff_y_history.append(current_diff)
                
                # ìˆ˜ì¶•/ì´ì™„ ìƒíƒœ íŒë³„ (ìŠ¤ë¬´ë”© ì ìš©)
                if len(diff_y_history) >= smoothing_window:
                    smoothed_recent = np.mean(diff_y_history[-smoothing_window:])
                    smoothed_prev = np.mean(diff_y_history[-smoothing_window-1:-1]) if len(diff_y_history) > smoothing_window else smoothed_recent
                    
                    if smoothed_recent > smoothed_prev + 3:  # ì°¨ì´ê°€ ì»¤ì§€ë©´ ìˆ˜ì¶• (ë‚´ë ¤ê°€ëŠ” ì¤‘)
                        new_phase = 'contraction'
                    elif smoothed_recent < smoothed_prev - 3:  # ì°¨ì´ê°€ ì‘ì•„ì§€ë©´ ì´ì™„ (ì˜¬ë¼ê°€ëŠ” ì¤‘)
                        new_phase = 'relaxation'
                    else:
                        new_phase = current_phase  # ìœ ì§€
                    
                    if new_phase != current_phase and new_phase is not None:
                        phase_start_frame = frame_count
                        current_phase = new_phase

                # ë¹„ë””ì˜¤ì— ì‹œê°ì  í”¼ë“œë°± ì¶”ê°€
                if save_video_path is not None:
                    # 1ï¸âƒ£ ë°œë ê¸°ì¤€ ìˆ˜ì§ì„  (ë¬´ë¦ì´ ë„˜ìœ¼ë©´ ë¹¨ê°„ìƒ‰, ì•ˆë„˜ìœ¼ë©´ ì´ˆë¡ìƒ‰)
                    line_color = (0, 0, 255) if over_distance > 0 else (0, 255, 0)
                    cv2.line(frame, (int(front_foot_x), 0), (int(front_foot_x), height), line_color, 2)
                    
                    # 2ï¸âƒ£ ìˆ˜ì§ì„  ì •ë ¬ ì‹œê°í™” (ì–´ê¹¨-ì—‰ë©ì´-ë°˜ëŒ€ìª½ ë¬´ë¦)
                    if vertical_deviation <= 10:
                        vertical_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (ì¢‹ìŒ)
                    elif vertical_deviation <= 20:
                        vertical_color = (0, 255, 255)  # ë…¸ë€ìƒ‰ (ì¤‘ê°„)
                    else:
                        vertical_color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (ë‚˜ì¨)
                    
                    # ì–´ê¹¨-ì—‰ë©ì´-ë°˜ëŒ€ìª½ ë¬´ë¦ì„ ì—°ê²°í•˜ëŠ” ì„ 
                    cv2.line(frame, (int(opp_shoulder[0]), int(opp_shoulder[1])), 
                             (int(opp_hip[0]), int(opp_hip[1])), vertical_color, 3)
                    cv2.line(frame, (int(opp_hip[0]), int(opp_hip[1])), 
                             (int(opp_knee_pt[0]), int(opp_knee_pt[1])), vertical_color, 3)
                    
                    # ì„¸ ì ì— ì› í‘œì‹œ
                    cv2.circle(frame, (int(opp_shoulder[0]), int(opp_shoulder[1])), 8, vertical_color, -1)
                    cv2.circle(frame, (int(opp_hip[0]), int(opp_hip[1])), 8, vertical_color, -1)
                    cv2.circle(frame, (int(opp_knee_pt[0]), int(opp_knee_pt[1])), 8, vertical_color, -1)
                    
                    # 3ï¸âƒ£ ê°€ë™ë²”ìœ„ ì‹œê°í™” (ì—‰ë©ì´-ë¬´ë¦ ë†’ì´ ì°¨ì´)
                    hip_y_val = int(hip[1])
                    knee_y_val = int(knee[1])
                    range_diff = knee_y_val - hip_y_val
                    
                    if abs(range_diff) <= 25:
                        range_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (ì¢‹ìŒ)
                    elif abs(range_diff) <= 50:
                        range_color = (0, 255, 255)  # ë…¸ë€ìƒ‰ (ì¤‘ê°„)
                    else:
                        range_color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (ë‚˜ì¨)
                    
                    # ì—‰ë©ì´ ë†’ì´ ê°€ë¡œì„ 
                    cv2.line(frame, (0, hip_y_val), (width, hip_y_val), (255, 165, 0), 2)
                    # ë¬´ë¦ ë†’ì´ ê°€ë¡œì„ 
                    cv2.line(frame, (0, knee_y_val), (width, knee_y_val), range_color, 2)
                    # ì—‰ë©ì´-ë¬´ë¦ ë†’ì´ ì°¨ì´ë¥¼ ì„¸ë¡œì„ ìœ¼ë¡œ í‘œì‹œ
                    mid_x = int((hip[0] + knee[0]) / 2)
                    cv2.line(frame, (mid_x, hip_y_val), (mid_x, knee_y_val), range_color, 3)
                    
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    mp.solutions.drawing_utils.draw_landmarks(
                        frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS
                    )
                    
                    # ========== í†µí•© ì •ë³´ ë°•ìŠ¤ (í•œê¸€ ì§€ì›) ==========
                    current_accuracy = max(0, 100 - (total_penalty / frame_count if frame_count > 0 else 0))
                    
                    # 4ï¸âƒ£ ìˆ˜ì¶•/ì´ì™„ ìƒíƒœ ê³„ì‚°
                    phase_duration = 0
                    phase_color = (255, 255, 255)
                    phase_korean = "-"
                    if current_phase is not None:
                        phase_duration = (frame_count - phase_start_frame) / fps
                        if 2.0 <= phase_duration <= 3.0:
                            phase_color = (0, 255, 0)
                        elif phase_duration < 2.0:
                            phase_color = (0, 255, 255)
                        else:
                            phase_color = (0, 0, 255)
                        phase_korean = "ìˆ˜ì¶• â†“" if current_phase == 'contraction' else "ì´ì™„ â†‘"
                    
                    # í†µí•© ì •ë³´ ë°•ìŠ¤ ë°°ê²½
                    box_height = 160 if current_phase else 120
                    cv2.rectangle(frame, (10, 10), (280, 10 + box_height), (40, 40, 40), -1)
                    cv2.rectangle(frame, (10, 10), (280, 10 + box_height), (100, 100, 100), 2)
                    
                    # í•œê¸€ í…ìŠ¤íŠ¸ ë Œë”ë§
                    # ë¬´ë¦ ì •í™•ë„ ìƒ‰ìƒ
                    acc_color = (0, 255, 0) if current_accuracy >= 90 else ((0, 255, 255) if current_accuracy >= 70 else (0, 0, 255))
                    
                    frame = put_korean_text(frame, f"ì •í™•ë„: {current_accuracy:.1f}%", (20, 15), font_size=22, color=acc_color)
                    frame = put_korean_text(frame, f"ìˆ˜ì§ ì •ë ¬: {vertical_deviation:.1f}Â°", (20, 45), font_size=20, color=vertical_color)
                    frame = put_korean_text(frame, f"ê°€ë™ë²”ìœ„: {abs(range_diff)}px", (20, 75), font_size=20, color=range_color)
                    
                    if current_phase is not None:
                        frame = put_korean_text(frame, f"ë™ì‘: {phase_korean}", (20, 105), font_size=20, color=phase_color)
                        frame = put_korean_text(frame, f"ì‹œê°„: {phase_duration:.1f}ì´ˆ", (20, 135), font_size=20, color=phase_color)

                if show_video:
                    cv2.line(frame, (int(front_foot_x), 0), (int(front_foot_x), height), (0, 0, 255), 2)
                    y = int(knee[1])
                    cv2.line(frame, (0, y), (width, y), (0, 255, 255), 2)
                    mp.solutions.drawing_utils.draw_landmarks(
                        frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS
                    )
                    cv2.imshow("Lunge Video", frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
            # ëœë“œë§ˆí¬ê°€ ìˆë“  ì—†ë“  í”„ë ˆì„ ì €ì¥ (ì—†ìœ¼ë©´ ì›ë³¸ í”„ë ˆì„ ì €ì¥)
            if video_writer is not None:
                video_writer.write(frame)

    if show_video:
        cv2.destroyAllWindows()
    if video_writer is not None:
        video_writer.release()
    return knee_xs, foot_xs, knee_angles, hip_y_list, knee_y_list

def calc_penalty(over_distance, threshold=10, max_penalty=100):
    x = max(0, over_distance)
    exp_input = min((x / threshold) ** 2, 10)
    penalty = math.exp(exp_input) - 1
    return min(penalty, max_penalty)

def plot_knee_foot_distance(knee_xs, foot_xs):
    distances = [knee - foot for knee, foot in zip(knee_xs, foot_xs)]
    penalties = []
    penalty_frames = []
    for idx, d in enumerate(distances):
        # ìŒìˆ˜(=ë¬´ë¦ì´ ë°œëë³´ë‹¤ ì•ìœ¼ë¡œ ë‚˜ê°)ì¼ ë•Œë§Œ íŒ¨ë„í‹° ë¶€ì—¬
        penalty = calc_penalty(-d) if d < 0 else 0
        penalties.append(penalty)
        if penalty > 0:
            penalty_frames.append(idx)
    avg_penalty = sum(penalties) / len(penalties) if penalties else 0
    accuracy = max(0, 100 - avg_penalty)

    plt.figure(figsize=(10, 5))
    plt.plot(distances, label="Knee - Foot X Distance")
    plt.axhline(0, color='red', linestyle='--', label='Foot X')
    # íŒ¨ë„í‹°ê°€ ë°œìƒí•œ í”„ë ˆì„ì— ë§ˆì»¤ í‘œì‹œ
    if penalty_frames:
        plt.scatter(np.array(penalty_frames), np.array([distances[i] for i in penalty_frames]), 
                    color='red', label='Penalty (Accuracyâ†“)', zorder=5)
    plt.xlabel("Frame")
    plt.ylabel("Knee X - Foot X (pixels)")
    plt.title(f"Distance from Foot to Knee (X axis) Over Frames\nAccuracy: {accuracy:.1f}%")
    plt.legend()
    plt.tight_layout()
    plt.show()

def calc_three_point_angle(a, b, c):
    # ê° bë¥¼ ê¸°ì¤€ìœ¼ë¡œ a-b-cì˜ ê°ë„ë¥¼ êµ¬í•¨
    ba = np.array([a[0] - b[0], a[1] - b[1]])
    bc = np.array([c[0] - b[0], c[1] - b[1]])
    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)
    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
    return np.degrees(angle)

def analyze_vertical_alignment(frame_gen):
    mp_pose = mp.solutions.pose
    width, height = None, None
    vertical_angles = []

    with mp_pose.Pose(static_image_mode=False) as pose:
        for frame in frame_gen:
            if width is None or height is None:
                height, width = frame.shape[:2]
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = pose.process(image_rgb)
            if not result.pose_landmarks:
                continue
            landmarks = result.pose_landmarks.landmark
            nose_x = landmarks[0].x * width
            right_shoulder_x = landmarks[12].x * width
            left_foot_x = landmarks[31].x * width
            right_foot_x = landmarks[32].x * width

            # ì•ë‹¤ë¦¬ íŒë³„
            if nose_x > right_shoulder_x:
                look_direction = "right"
            else:
                look_direction = "left"

            if look_direction == "right":
                if right_foot_x > left_foot_x:
                    hip_idx, shoulder_idx, opp_knee_idx = 24, 12, 25
                else:
                    hip_idx, shoulder_idx, opp_knee_idx = 23, 11, 26
            else:
                if left_foot_x < right_foot_x:
                    hip_idx, shoulder_idx, opp_knee_idx = 23, 11, 26
                else:
                    hip_idx, shoulder_idx, opp_knee_idx = 24, 12, 25

            hip = (landmarks[hip_idx].x * width, landmarks[hip_idx].y * height)
            shoulder = (landmarks[shoulder_idx].x * width, landmarks[shoulder_idx].y * height)
            opp_knee = (landmarks[opp_knee_idx].x * width, landmarks[opp_knee_idx].y * height)

            angle = calc_three_point_angle(shoulder, hip, opp_knee)
            vertical_angles.append(angle)

    return vertical_angles

def make_feedback_advanced(vertical_score, movement_speed, knee_accuracy, movement_range):
    feedback = []
    # ë¬´ë¦ ì •í™•ë„
    if knee_accuracy >= 90:
        feedback.append("ë¬´ë¦ì´ ë°œë ì•ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šì•˜ì–´ìš”. ì¢‹ì•„ìš”!")
    else:
        feedback.append("ë¬´ë¦ì´ ë°œë ì•ìœ¼ë¡œ ë‚˜ê°”ìŠµë‹ˆë‹¤. ì£¼ì˜í•˜ì„¸ìš”!")
    # ìˆ˜ì§ ì •ë ¬
    if vertical_score >= 90:
        feedback.append("ìˆ˜ì§ ì •ë ¬ì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.")
    elif vertical_score >= 60:
        feedback.append("ìˆ˜ì§ ì •ë ¬ì´ ì•½ê°„ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        feedback.append("ìˆ˜ì§ ì •ë ¬ì´ ë§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    # ê°€ë™ë²”ìœ„
    if movement_range >= 80:
        feedback.append("ê°€ë™ë²”ìœ„ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.")
    else:
        feedback.append("ê°€ë™ë²”ìœ„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ê¹Šê²Œ ë‚´ë ¤ê°€ë³´ì„¸ìš”.")
    # ìˆ˜ì¶•/ì´ì™„ ì†ë„
    if movement_speed["contractionPercent"] >= 80 and movement_speed["relaxationPercent"] >= 80:
        feedback.append("ìˆ˜ì¶•ê³¼ ì´ì™„ ì†ë„ê°€ ì ì ˆí•©ë‹ˆë‹¤.")
    else:
        if movement_speed["contractionPercent"] < 80:
            feedback.append(f"ìˆ˜ì¶• ì†ë„ê°€ ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‰ê·  ìˆ˜ì¶• ì‹œê°„: {movement_speed['avgContractionTime']:.2f}ì´ˆ")
        if movement_speed["relaxationPercent"] < 80:
            feedback.append(f"ì´ì™„ ì†ë„ê°€ ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‰ê·  ì´ì™„ ì‹œê°„: {movement_speed['avgRelaxationTime']:.2f}ì´ˆ")
    return "\n".join(feedback)

def find_contraction_relaxation(diff_y_list, fps, threshold=3):
    def smooth_signal(signal, window=7):
        return np.convolve(signal, np.ones(window)/window, mode='same')
    smoothed = smooth_signal(diff_y_list)
    direction = np.sign(np.diff(smoothed))
    transitions = []
    prev_dir = direction[0]
    for i in range(1, len(direction)):
        if direction[i] != prev_dir and abs(smoothed[i+1] - smoothed[i]) > threshold:
            transitions.append(i+1)
            prev_dir = direction[i]
    segments = []
    prev = 0
    for idx in transitions:
        # diffê°€ ì»¤ì§ˆ ë•Œ contraction, ì‘ì•„ì§ˆ ë•Œ relaxation (ì˜ìƒê³¼ ì¼ì¹˜)
        phase = 'contraction' if smoothed[idx] > smoothed[idx-1] else 'relaxation'
        segments.append((phase, prev, idx))
        prev = idx
    if prev < len(smoothed)-1:
        phase = 'contraction' if smoothed[-1] > smoothed[-2] else 'relaxation'
        if len(segments) == 0 or segments[-1][0] != phase:
            segments.append((phase, prev, len(smoothed)-1))
    # í‰ê·  ì‹œê°„ ê³„ì‚°
    contraction_times = []
    relaxation_times = []
    for phase, start, end in segments:
        duration_sec = (end - start) / fps if fps > 0 else 0
        if phase == 'contraction':
            contraction_times.append(duration_sec)
        elif phase == 'relaxation':
            relaxation_times.append(duration_sec)
    avg_contraction = np.mean(contraction_times) if contraction_times else 0
    avg_relaxation = np.mean(relaxation_times) if relaxation_times else 0
    # ì ì ˆì„± í¼ì„¼íŠ¸
    def calc_percent(avg_time):
        if 2.0 <= avg_time <= 3.0:
            return 100
        elif avg_time < 2.0:
            return int((avg_time / 2.0) * 100)
        else:
            return int((3.0 / avg_time) * 100)
    contraction_percent = calc_percent(avg_contraction)
    relaxation_percent = calc_percent(avg_relaxation)
    return avg_contraction, avg_relaxation, contraction_percent, relaxation_percent

def lunge_video_level3(video_bytes: bytes, feedback_id: int) -> dict:
    print("="*50)
    print("ğŸ¯ LEVEL 3 ë¶„ì„ ì‹œì‘ (ê³ ê¸‰)")
    print("ë¶„ì„ í•­ëª©: ë¬´ë¦-ë°œë ì •ë ¬, ìˆ˜ì§ì„  ì •ë ¬, ê°€ë™ë²”ìœ„, ìˆ˜ì¶•/ì´ì™„ ì†ë„")
    print("="*50)
    
    import tempfile
    import uuid
    from app.utils.minio_client import client as minio_client, bucket_name
    import app.utils.minio_client as minio_client_module

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as input_tmp:
        input_tmp.write(video_bytes)
    input_path = input_tmp.name

    # FPS ì¶”ì¶œ
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    cap.release()

    # 1ë‹¨ê³„: ë¶„ì„ê°’ ì¶”ì¶œ (ë¹„ë””ì˜¤ ìƒì„± ì—†ì´)
    frame_gen = rotated_frame_generator(input_path)
    knee_xs, foot_xs, knee_angles, hip_y_list, knee_y_list = extract_front_knee_foot_xs_lunge_style(frame_gen, show_video=False)

    # 2ë‹¨ê³„: ì‹œê°ì  í”¼ë“œë°±ì´ í¬í•¨ëœ ë¹„ë””ì˜¤ ìƒì„±
    output_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    output_path = output_tmp.name
    output_tmp.close()
    
    frame_gen_visual = rotated_frame_generator(input_path)
    extract_front_knee_foot_xs_lunge_style(frame_gen_visual, show_video=False, save_video_path=output_path, fps=fps)

    # --- ìˆ˜ì§ì„  ì •ë ¬ ë¶„ì„ ---
    vertical_angles = analyze_vertical_alignment(rotated_frame_generator(input_path))
    vertical_deviation = [abs(180 - angle) for angle in vertical_angles]
    n = len(vertical_deviation)
    k = max(1, int(n * 0.1))
    best_vertical = np.mean(sorted(vertical_deviation)[:k]) if k > 0 else 180

    vertical_tolerance = 10  # ë„
    if best_vertical <= vertical_tolerance:
        vertical_score = 100
        vertical_level = "ì¢‹ìŒ"
    elif best_vertical <= vertical_tolerance * 2:
        vertical_score = max(0, 100 - ((best_vertical - vertical_tolerance) / vertical_tolerance) * 100)
        vertical_level = "ì¤‘ê°„"
    else:
        vertical_score = 0
        vertical_level = "ë‚˜ì¨"
    vertical_score = min(vertical_score, 100)

    distances = [knee - foot for knee, foot in zip(knee_xs, foot_xs)]
    penalties = []
    penalty_frames = []
    for idx, d in enumerate(distances):
        penalty = calc_penalty(-d) if d < 0 else 0
        penalties.append(penalty)
        if penalty > 0:
            penalty_frames.append(idx)
    avg_penalty = sum(penalties) / len(penalties) if penalties else 0
    knee_accuracy = max(0, 100 - avg_penalty)

    # ê¸°ì¡´ movement_range ê³„ì‚° (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    sorted_diffs = sorted([h - k for h, k in zip(hip_y_list, knee_y_list)])
    n = len(sorted_diffs)
    k = max(1, int(n * 0.1))
    movement_range = round(np.mean(sorted_diffs[-k:]), 2)

    # test.pyì™€ ë™ì¼í•œ ê°€ë™ë²”ìœ„ í‰ê°€ ê³µì‹
    diff_y_list = [abs(knee - hip) for knee, hip in zip(knee_y_list, hip_y_list)]
    sorted_diffs = sorted(diff_y_list, key=lambda x: abs(x))
    n = len(sorted_diffs)
    k = max(1, int(n * 0.1))
    best_range_avg = np.mean(sorted_diffs[:k]) if k > 0 else 0

    tolerance = 25  # í”½ì…€
    if abs(best_range_avg) <= tolerance:
        score = 100
        level = "ì¢‹ìŒ"
    elif abs(best_range_avg) <= tolerance * 2:
        score = max(0, 100 - ((abs(best_range_avg) - tolerance) / tolerance) * 100)
        level = "ì¤‘ê°„"
    else:
        score = 0
        level = "ë‚˜ì¨"
    score = min(score, 100)

    # --- ìˆ˜ì¶•/ì´ì™„ ì†ë„ ë¶„ì„ ì¶”ê°€ ---
    cap = cv2.VideoCapture(input_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    cap.release()
    if fps <= 0 or frame_count <= 0:
        frame_count = len(knee_y_list)
        fps = frame_count / (len(knee_y_list) / 30) if len(knee_y_list) > 0 else 30

    avg_contraction, avg_relaxation, contraction_percent, relaxation_percent = find_contraction_relaxation(diff_y_list, fps)

    movementSpeed = {
        "avgContractionTime": round(avg_contraction, 2),
        "avgRelaxationTime": round(avg_relaxation, 2),
        "contractionPercent": contraction_percent,
        "relaxationPercent": relaxation_percent
    }

    # 3ë‹¨ê³„: FFmpegë¡œ ë¸Œë¼ìš°ì € ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” (faststart)
    import subprocess
    optimized_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    optimized_path = optimized_tmp.name
    optimized_tmp.close()
    
    try:
        # FFmpegë¡œ H.264 ì¬ì¸ì½”ë”© + faststart (moov atomì„ íŒŒì¼ ì•ìœ¼ë¡œ)
        subprocess.run([
            'ffmpeg', '-i', output_path,
            '-c:v', 'libx264',  # H.264 ì½”ë±
            '-preset', 'fast',  # ë¹ ë¥¸ ì¸ì½”ë”©
            '-movflags', '+faststart',  # ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
            '-y',  # ë®ì–´ì“°ê¸°
            optimized_path
        ], check=True, capture_output=True)
        
        # ìµœì í™”ëœ íŒŒì¼ë¡œ êµì²´
        final_output = optimized_path
        print(f"Optimized video with FFmpeg: {final_output}")
    except subprocess.CalledProcessError as e:
        print(f"FFmpeg optimization failed: {e.stderr.decode()}")
        # FFmpeg ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
        final_output = output_path
    except FileNotFoundError:
        print("FFmpeg not found, using original video")
        final_output = output_path

    # (4) MinIOì— ì—…ë¡œë“œ
    bucket_name = "levelupfit-videos"
    object_name = f"{uuid.uuid4()}.mp4"
    try:
        import os
        file_size = os.path.getsize(final_output)
        with open(final_output, 'rb') as file_data:
            minio_client.put_object(
                bucket_name=bucket_name,
                object_name=object_name,
                data=file_data,
                length=file_size,
                content_type="video/mp4"
            )
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        import os
        if os.path.exists(input_path):
            os.remove(input_path)
        if os.path.exists(output_path):
            os.remove(output_path)
        if 'optimized_path' in locals() and os.path.exists(optimized_path) and optimized_path != final_output:
            os.remove(optimized_path)
    video_url = f"https://{minio_client_module.MINIO_URL}/{bucket_name}/{object_name}"
    accuracy = (knee_accuracy + vertical_score) / 2
    
    # LLM í”¼ë“œë°± ìƒì„± (ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback)
    from app.services.llm_feedback import generate_feedback_level3
    feedback_text = generate_feedback_level3(
        accuracy=accuracy,
        movement_range=round(score, 1),
        knee_accuracy=knee_accuracy,
        vertical_score=vertical_score,
        movement_speed=movementSpeed
    )
    print(round(score, 1), level, round(best_range_avg, 2), round(vertical_score, 1), vertical_level, round(best_vertical, 2))

    return {
        "feedback_id": feedback_id,
        "video_url": video_url,
        "feedback_text": feedback_text,
        "accuracy": accuracy,
        "movementRange": round(score, 1),
        "movementSpeed": {
            "avgContractionTime": round(avg_contraction, 2),
            "avgRelaxationTime": round(avg_relaxation, 2),
            "contractionPercent": contraction_percent,
            "relaxationPercent": relaxation_percent
        }
        # "rangeScore": round(score, 1),
        # "rangeLevel": level,
        # "rangeDiffAvg": round(best_range_avg, 2),
        # "verticalScore": round(vertical_score, 1),
        # "verticalLevel": vertical_level,
        # "verticalDeviationAvg": round(best_vertical, 2)
    }

